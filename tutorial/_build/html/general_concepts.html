

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2. Machine Learning 101 &mdash; scikit-learn tutorial v0.9 documentation</title>
    <link rel="stylesheet" href="static/nature.css" type="text/css" />
    <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.9',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="static/jquery.js"></script>
    <script type="text/javascript" src="static/underscore.js"></script>
    <script type="text/javascript" src="static/doctools.js"></script>
    <link rel="shortcut icon" href="static/favicon.ico"/>
    <link rel="top" title="scikit-learn tutorial v0.9 documentation" href="index.html" />
    <link rel="next" title="3. Working with text data" href="working_with_text_data.html" />
    <link rel="prev" title="1. Tutorial setup" href="setup.html" /> 
  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="index.html">
            <img src="static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="http://github.com/scikit-learn/scikit-learn-tutorial">Source of this tutorial</a></li>
            <li><a href="http://scikit-learn.sourceforge.net">Documentation for scikit-learn</a></li>
       </ul>


       </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <!-- <div id="blue_tile"></div> -->

        <div class="sphinxsidebar">
        <div class="rel">
          <a href="setup.html" title="1. Tutorial setup"
             accesskey="P">previous</a> |
          <a href="working_with_text_data.html" title="3. Working with text data"
             accesskey="N">next</a> |
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
        

        <h3>Contents</h3>
         <ul>
<li><a class="reference internal" href="#">2. Machine Learning 101</a><ul>
<li><a class="reference internal" href="#features-and-feature-extraction">2.1. Features and feature extraction</a><ul>
<li><a class="reference internal" href="#a-simple-example-the-iris-dataset">2.1.1. A simple example: the iris dataset</a></li>
<li><a class="reference internal" href="#handling-categorical-features">2.1.2. Handling categorical features</a></li>
<li><a class="reference internal" href="#extracting-features-from-unstructured-data">2.1.3. Extracting features from unstructured data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#supervised-learning-model-fit-x-y">2.2. Supervised Learning: <tt class="docutils literal"><span class="pre">model.fit(X,</span> <span class="pre">y)</span></tt></a><ul>
<li><a class="reference internal" href="#classification">2.2.1. Classification</a><ul>
<li><a class="reference internal" href="#a-first-classifier-example-with-scikit-learn">2.2.1.1. A first classifier example with <tt class="docutils literal"><span class="pre">scikit-learn</span></tt></a></li>
<li><a class="reference internal" href="#notable-implementations-of-classifiers">2.2.1.2. Notable implementations of classifiers</a></li>
<li><a class="reference internal" href="#sample-application-of-classifiers">2.2.1.3. Sample application of classifiers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression">2.2.2. Regression</a></li>
</ul>
</li>
<li><a class="reference internal" href="#unsupervised-learning-model-fit-x">2.3. Unsupervised Learning: <tt class="docutils literal"><span class="pre">model.fit(X)</span></tt></a><ul>
<li><a class="reference internal" href="#dimensionality-reduction-and-visualization">2.3.1. Dimensionality Reduction and visualization</a><ul>
<li><a class="reference internal" href="#normalization-and-visualization-with-pca">2.3.1.1. Normalization and visualization with PCA</a></li>
<li><a class="reference internal" href="#other-applications-of-dimensionality-reduction">2.3.1.2. Other applications of dimensionality reduction</a></li>
</ul>
</li>
<li><a class="reference internal" href="#clustering">2.3.2. Clustering</a><ul>
<li><a class="reference internal" href="#notable-implementations-of-clustering-models">2.3.2.1. Notable implementations of clustering models</a></li>
<li><a class="reference internal" href="#applications-of-clustering">2.3.2.2. Applications of clustering</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#linearly-separable-data">2.4. Linearly separable data</a></li>
<li><a class="reference internal" href="#training-set-test-set-and-overfitting">2.5. Training set, test set and overfitting</a><ul>
<li><a class="reference internal" href="#the-overfitting-issue">2.5.1. The overfitting issue</a></li>
<li><a class="reference internal" href="#solutions-to-overfitting">2.5.2. Solutions to overfitting</a></li>
<li><a class="reference internal" href="#measuring-classification-performance-on-a-test-set">2.5.3. Measuring classification performance on a test set</a></li>
</ul>
</li>
<li><a class="reference internal" href="#key-takeaway-points">2.6. Key takeaway points</a></li>
</ul>
</li>
</ul>


      </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="machine-learning-101">
<h1>2. Machine Learning 101<a class="headerlink" href="#machine-learning-101" title="Permalink to this headline">¶</a></h1>
<p>Machine Learning is about building <strong>programs with tunable parameters</strong>
(typically an array of floating point values) that are adjusted
automatically so as to improve their behavior by <strong>adapting to
previously seen data</strong>.</p>
<p>Machine Learning can be considered a <strong>subfield of Artificial
Intelligence</strong> since those algorithms can be seen as building blocks
to make computers learn to behave more intelligently by somehow
<strong>generalizing</strong> rather that just storing and retrieving data items
like a database system would do.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/decision_boundary.png"><img alt="images/decision_boundary.png" src="images/decision_boundary.png" style="width: 256.0px; height: 200.5px;" /></a>
<p class="caption">Decision boundary learned from data points from two categories:
white and black</p>
</div>
<p>The following will introduce the main concepts used to qualify
machine learning algorithms as implemented in <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>:</p>
<ul class="simple">
<li>how to turn raw data info numerical arrays</li>
<li>what is supervised learning</li>
<li>what is unsupervised learning</li>
<li>what is linearly separable data</li>
<li>what is overfitting</li>
</ul>
<div class="section" id="features-and-feature-extraction">
<h2>2.1. Features and feature extraction<a class="headerlink" href="#features-and-feature-extraction" title="Permalink to this headline">¶</a></h2>
<p>Most machine learning algorithms implemented in <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>
expect a numpy array as input <tt class="docutils literal"><span class="pre">X</span></tt>.  The expected shape of <tt class="docutils literal"><span class="pre">X</span></tt> is
<tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name"><tt class="docutils literal"><span class="pre">n_samples</span></tt>:</th><td class="field-body">The number of samples: each sample is an item to process (e.g.
classify). A sample can be a document, a picture, a sound, a
video, a row in database or CSV file, or whatever you can
describe with a fixed set of quantitative traits.</td>
</tr>
<tr class="field"><th class="field-name"><tt class="docutils literal"><span class="pre">n_features</span></tt>:</th><td class="field-body">The number of features or distinct traits that can be used to
describe each item in a quantitative manner.</td>
</tr>
</tbody>
</table>
<p>The number of features must be fixed in advance. However it can be
very high dimensional (e.g. millions of features) with most of them
being zeros for a given sample. In this case we use <tt class="docutils literal"><span class="pre">scipy.sparse</span></tt>
matrices instead of <tt class="docutils literal"><span class="pre">numpy</span></tt> arrays so as to make the data fit
in memory.</p>
<div class="section" id="a-simple-example-the-iris-dataset">
<h3>2.1.1. A simple example: the iris dataset<a class="headerlink" href="#a-simple-example-the-iris-dataset" title="Permalink to this headline">¶</a></h3>
<p>The machine learning community often uses a simple flowers database
where each row in the database (or CSV file) is a set of measurements
of an individual iris flower.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/Virginia_Iris.png"><img alt="Photo of Iris Virginia" src="images/Virginia_Iris.png" style="width: 200.0px; height: 162.0px;" /></a>
<p class="caption">Iris Virginia (source: Wikipedia)</p>
</div>
<p>Each sample in this dataset is described by 4 features and can
belong to one of the target classes:</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2">Features in the Iris dataset:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body"><ol class="first arabic simple" start="0">
<li>sepal length in cm</li>
<li>sepal width in cm</li>
<li>petal length in cm</li>
<li>petal width in cm</li>
</ol>
</td>
</tr>
<tr class="field"><th class="field-name" colspan="2">Target classes to predict:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body"><ol class="first last arabic simple" start="0">
<li>Iris Setosa</li>
<li>Iris Versicolour</li>
<li>Iris Virginica</li>
</ol>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p><tt class="docutils literal"><span class="pre">scikit-learn</span></tt> embeds a copy of the iris CSV file along with a
helper function to load it into numpy arrays:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To be able to copy and paste examples without taking care of the leading
<tt class="docutils literal"><span class="pre">&gt;&gt;&gt;</span></tt> and <tt class="docutils literal"><span class="pre">...</span></tt> prompt signs, enable the ipython doctest mode with:
<tt class="docutils literal"><span class="pre">%doctest_mode</span></tt></p>
</div>
<p>The features of each sample flower are stored in the <tt class="docutils literal"><span class="pre">data</span></tt> attribute
of the dataset:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span>
<span class="go">150</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">n_features</span>
<span class="go">4</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">array([ 5.1,  3.5,  1.4,  0.2])</span>
</pre></div>
</div>
<p>The information about the class of each sample is stored in the
<tt class="docutils literal"><span class="pre">target</span></tt> attribute of the dataset:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_samples</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="go">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="go">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="go">       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="go">       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="go">       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="go">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="go">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
</pre></div>
</div>
<p>The names of the classes are stored in the last attribute, namely
<tt class="docutils literal"><span class="pre">target_names</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="go">[&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="handling-categorical-features">
<h3>2.1.2. Handling categorical features<a class="headerlink" href="#handling-categorical-features" title="Permalink to this headline">¶</a></h3>
<p>Sometimes people describe samples with categorical descriptors that
have no obvious numerical representation. For instance assume that
each flower is further described by a color name among a fixed list
of color names:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">color</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;purple&#39;</span><span class="p">,</span> <span class="s">&#39;blue&#39;</span><span class="p">,</span> <span class="s">&#39;red&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>The simple way to turn this categorical feature into numerical
features suitable for machine learning is to create new features
for each distinct color name that can be valued to <tt class="docutils literal"><span class="pre">1.0</span></tt> if the
category is matching or <tt class="docutils literal"><span class="pre">0.0</span></tt> if not.</p>
<p>The enriched iris feature set would hence be in this case:</p>
<blockquote>
<div><ol class="arabic simple" start="0">
<li>sepal length in cm</li>
<li>sepal width in cm</li>
<li>petal length in cm</li>
<li>petal width in cm</li>
<li>color#purple (1.0 or 0.0)</li>
<li>color#blue (1.0 or 0.0)</li>
<li>color#red (1.0 or 0.0)</li>
</ol>
</div></blockquote>
</div>
<div class="section" id="extracting-features-from-unstructured-data">
<h3>2.1.3. Extracting features from unstructured data<a class="headerlink" href="#extracting-features-from-unstructured-data" title="Permalink to this headline">¶</a></h3>
<p>The previous example deals with features that are readily available
in a structured dataset with rows and columns of numerical or
categorical values.</p>
<p>However, <strong>most of the produced data is not readily available in a
structured representation</strong> such as SQL, CSV, XML, JSON or RDF.</p>
<p>Here is an overview of strategies to turn unstructed data items
into arrays of numerical features.</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Text documents:</th><td class="field-body"><p class="first">Count the frequency of each word or pair of consecutive words
in each document. This approach is called <strong>Bag of Words</strong>.</p>
<p>Note: we include other file formats such as HTML and PDF in
this category: an ad-hoc preprocessing step is required to
extract the plain text in UTF-8 encoding for instance.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Images:</th><td class="field-body"><ul class="first">
<li><p class="first">Rescale the picture to a fixed size and <strong>take all the raw
pixels values</strong> (with or without luminosity normalization)</p>
</li>
<li><p class="first">Take some transformation of the signal (gradients in each
pixel, wavelets transforms...)</p>
</li>
<li><p class="first">Compute the Euclidean, Manhattan or cosine <strong>similarities of
the sample to a set reference prototype images</strong> aranged in a
code book.  The code book may have been previously extracted
from the same dataset using an unsupervised learning algorithm
on the raw pixel signal.</p>
<p>Each feature value is the distance to one element of the code
book.</p>
</li>
<li><p class="first">Perform <strong>local feature extraction</strong>: split the picture into
small regions and perform feature extraction locally in each
area.</p>
<p>Then combine all the features of the individual areas into a
single array.</p>
</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Sounds:</th><td class="field-body"><p class="first last">Same strategy as for images within a 1D space instead of 2D</p>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Practical implementations of such feature extraction strategies
will be presented in the last sections of this tutorial.</p>
</div>
</div>
<div class="section" id="supervised-learning-model-fit-x-y">
<h2>2.2. Supervised Learning: <tt class="docutils literal"><span class="pre">model.fit(X,</span> <span class="pre">y)</span></tt><a class="headerlink" href="#supervised-learning-model-fit-x-y" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/supervised.png"><img alt="Flow diagram for supervised learning" src="images/supervised.png" style="width: 517.5px; height: 317.25px;" /></a>
<p class="caption">Supervised Learning overview</p>
</div>
<p>A supervised learning algorithm makes the distinction between the
raw observed data <tt class="docutils literal"><span class="pre">X</span></tt> with shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt> and
some label given to the model while training by some teacher. In
<tt class="docutils literal"><span class="pre">scikit-learn</span></tt> this array is often noted <tt class="docutils literal"><span class="pre">y</span></tt> and has generally
the shape <tt class="docutils literal"><span class="pre">(n_samples,)</span></tt>.</p>
<p>After training, the fitted model does no longer expect the <tt class="docutils literal"><span class="pre">y</span></tt>
as an input: it will try to predict the most likely labels <tt class="docutils literal"><span class="pre">y_new</span></tt>
for new a set of samples <tt class="docutils literal"><span class="pre">X_new</span></tt>.</p>
<p>Depending on the nature of the target <tt class="docutils literal"><span class="pre">y</span></tt>, supervised learning
can be given different names:</p>
<blockquote>
<div><ul class="simple">
<li>If <tt class="docutils literal"><span class="pre">y</span></tt> has values in a fixed set of <strong>categorical outcomes</strong>
(represented by <strong>integers</strong>) the task to predict <tt class="docutils literal"><span class="pre">y</span></tt> is called
<strong>classification</strong>.</li>
<li>If <tt class="docutils literal"><span class="pre">y</span></tt> has <strong>floating point values</strong> (e.g. to represent a price,
a temperature, a size...), the task to predict <tt class="docutils literal"><span class="pre">y</span></tt> is called
<strong>regression</strong>.</li>
</ul>
</div></blockquote>
<div class="section" id="classification">
<h3>2.2.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<div class="section" id="a-first-classifier-example-with-scikit-learn">
<h4>2.2.1.1. A first classifier example with <tt class="docutils literal"><span class="pre">scikit-learn</span></tt><a class="headerlink" href="#a-first-classifier-example-with-scikit-learn" title="Permalink to this headline">¶</a></h4>
<p>In the iris dataset example, suppose we are assigned the task to
guess the class of an individual flower given the measurements of
petals and sepals. This is a classification task, hence we have:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>Once the data has this format it is trivial to train a classifier,
for instance a support vector machine with a linear kernel:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Whenever you import a scikit-learn class or function for the first time,
you are advised to read the docstring by using the <tt class="docutils literal"><span class="pre">?</span></tt> magic suffix
of ipython, for instance type: <tt class="docutils literal"><span class="pre">LinearSVC?</span></tt>.</p>
</div>
<p><tt class="docutils literal"><span class="pre">clf</span></tt> is a statistical model that has parameters that control the
learning algorithm (those parameters are sometimes called the
hyperparameters). Those hyperparameters can be supplied by the
user in the constructor of the model. We will explain later how to choose
a good combination using either simple empirical rules or data
driven selection:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span>
<span class="go">LinearSVC(loss=&#39;l2&#39;, C=1.0, dual=True, fit_intercept=True, penalty=&#39;l2&#39;,</span>
<span class="go">     multi_class=False, tol=0.0001, intercept_scaling=1)</span>
</pre></div>
</div>
<p>By default the real model parameters are not initialized. They will be
tuned automatically from the data by calling the <tt class="docutils literal"><span class="pre">fit</span></tt> method:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>                         
<span class="go">array([[ 0.18...,  0.45..., -0.80..., -0.45...],</span>
<span class="go">       [ 0.05..., -0.89...,  0.40..., -0.93...],</span>
<span class="go">       [-0.85..., -0.98...,  1.38...,  1.86...]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>                    
<span class="go">array([ 0.10...,  1.67..., -1.70...])</span>
</pre></div>
</div>
<p>Once the model is trained, it can be used to predict the most likely outcome on
unseen data. For instance let us define a list of simple sample that looks
like the first sample of the iris dataset:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">5.0</span><span class="p">,</span>  <span class="mf">3.6</span><span class="p">,</span>  <span class="mf">1.3</span><span class="p">,</span>  <span class="mf">0.25</span><span class="p">]]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="go">array([0], dtype=int32)</span>
</pre></div>
</div>
<p>The outcome is <tt class="docutils literal"><span class="pre">0</span></tt> which is the id of the first iris class, namely
&#8216;setosa&#8217;.</p>
<p>The following figure places the location of the <tt class="docutils literal"><span class="pre">fit</span></tt> and <tt class="docutils literal"><span class="pre">predict</span></tt>
calls on the previous flow diagram. The <tt class="docutils literal"><span class="pre">vec</span></tt> object is a vectorizer
used for feature extraction that is not used in the case of the iris
data (it already comes as vectors of features):</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/supervised_scikit_learn.png"><img alt="Flow diagram for supervised learning with scikit-learn" src="images/supervised_scikit_learn.png" style="width: 494.25px; height: 363.0px;" /></a>
<p class="caption">Supervised Learning with scikit-learn</p>
</div>
<p>Some <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> classifiers can further predict probabilities
of the outcome.  This is the case of logistic regression models:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span>
<span class="go">LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,</span>
<span class="go">          penalty=&#39;l2&#39;, tol=0.0001)</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="go">array([[  9.07512928e-01,   9.24770379e-02,   1.00343962e-05]])</span>
</pre></div>
</div>
<p>This means that the model estimates that the sample in <tt class="docutils literal"><span class="pre">X_new</span></tt> has:</p>
<blockquote>
<div><ul class="simple">
<li>90% likelyhood to belong to the &#8216;setosa&#8217; class</li>
<li>9% likelyhood to belong to the &#8216;versicolor&#8217; class</li>
<li>1% likelyhood to belong to the &#8216;virginica&#8217; class</li>
</ul>
</div></blockquote>
<p>Of course, the <tt class="docutils literal"><span class="pre">predict</span></tt> method that outputs the label id of the
most likely outcome is also available:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="go">array([0], dtype=int32)</span>
</pre></div>
</div>
</div>
<div class="section" id="notable-implementations-of-classifiers">
<h4>2.2.1.2. Notable implementations of classifiers<a class="headerlink" href="#notable-implementations-of-classifiers" title="Permalink to this headline">¶</a></h4>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.LogisticRegression</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Regularized Logistic Regression based on <tt class="docutils literal"><span class="pre">liblinear</span></tt></td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.svm.LinearSVC</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Support Vector Machines without kernels based on <tt class="docutils literal"><span class="pre">liblinear</span></tt></td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.svm.SVC</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Support Vector Machines with kernels based on <tt class="docutils literal"><span class="pre">libsvm</span></tt></td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.SGDClassifier</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Regularized linear models (SVM or logistic regression) using a Stochastic
Gradient Descent algorithm written in <tt class="docutils literal"><span class="pre">Cython</span></tt></td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.neighbors.NeighborsClassifier</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">k-Nearest Neighbors classifier based on the ball tree datastructure for low
dimensional data and brute force search for high dimensional data</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="sample-application-of-classifiers">
<h4>2.2.1.3. Sample application of classifiers<a class="headerlink" href="#sample-application-of-classifiers" title="Permalink to this headline">¶</a></h4>
<p>The following table gives examples of applications of classifiers
for some common engineering tasks:</p>
<table border="1" class="docutils">
<colgroup>
<col width="57%" />
<col width="43%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Task</th>
<th class="head">Predicted outcomes</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>E-mail classification</td>
<td>Spam, normal, priority mail</td>
</tr>
<tr><td>Language identification in text documents</td>
<td>en, es, de, fr, ja, zh, ar, ru...</td>
</tr>
<tr><td>News articles categorization</td>
<td>Business, technology, sports...</td>
</tr>
<tr><td>Sentiment analysis in customer feedback</td>
<td>Negative, neutral, positive</td>
</tr>
<tr><td>Face verification in pictures</td>
<td>Same / different person</td>
</tr>
<tr><td>Speaker verification in voice recordings</td>
<td>Same / different person</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="regression">
<h3>2.2.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h3>
<p>Regression is the task of predicting the value of a continuously varying
variable (e.g. a price, a temperature, a conversion rate...) given
some input variables (a.k.a. the features, &#8220;predictors&#8221; or
&#8220;regressors&#8221;). Some notable implementations of regression models in
<tt class="docutils literal"><span class="pre">scikit-learn</span></tt> include:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.Ridge</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">L2-regularized least squares linear model</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.ElasticNet</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">L1+L2-regularized least squares linear model trained using
Coordinate Descent</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.LassoLARS</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">L1-regularized least squares linear model trained with Least Angle
Regression</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.SGDRegressor</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">L1+L2-regularized least squares linear model trained using
Stochastic Gradient Descent</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.linear_model.ARDRegression</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Bayesian Automated Relevance Determination regression</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.svm.SVR</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Non-linear regression using Support Vector Machines (wrapper for
<tt class="docutils literal"><span class="pre">libsvm</span></tt>)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="unsupervised-learning-model-fit-x">
<h2>2.3. Unsupervised Learning: <tt class="docutils literal"><span class="pre">model.fit(X)</span></tt><a class="headerlink" href="#unsupervised-learning-model-fit-x" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/unsupervised.png"><img alt="Flow diagram for unsupervised learning" src="images/unsupervised.png" style="width: 522.0px; height: 302.25px;" /></a>
<p class="caption">Unsupervised Learning overview</p>
</div>
<p>An unsupervised learning algorithm only uses a single set of
observations <tt class="docutils literal"><span class="pre">X</span></tt> with shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt> and does
not use any kind of labels.</p>
<p>An unsupervised learning model will try to fit its parameters so
as to best summarize regularities found in the data.</p>
<p>The following introduces the main variants of unsupervised learning
algorithms, namely dimensionality reduction and clustering.</p>
<div class="section" id="dimensionality-reduction-and-visualization">
<h3>2.3.1. Dimensionality Reduction and visualization<a class="headerlink" href="#dimensionality-reduction-and-visualization" title="Permalink to this headline">¶</a></h3>
<p>Dimensionality reduction is the task of deriving a set of <strong>new artificial
features</strong> that is <strong>smaller</strong> than the original feature set while
retaining <strong>most of the variance</strong> of the original data.</p>
<div class="section" id="normalization-and-visualization-with-pca">
<h4>2.3.1.1. Normalization and visualization with PCA<a class="headerlink" href="#normalization-and-visualization-with-pca" title="Permalink to this headline">¶</a></h4>
<p>The most common technique for dimensionality reduction is called
<strong>Principal Component Analysis</strong>.</p>
<p>PCA can be done using linear combinations of the original features
using a truncated Singular Value Decomposition of the matrix <tt class="docutils literal"><span class="pre">X</span></tt>
so as to project the data onto a base of the top singular vectors.</p>
<p>If the number of retained components is 2 or 3, PCA can be used to
visualize the dataset:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>Once fitted, the <tt class="docutils literal"><span class="pre">pca</span></tt> model exposes the singular vectors in the
<tt class="docutils literal"><span class="pre">components_</span></tt> attribute:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span>                                      
<span class="go">array([[ 0.17..., -0.04...,  0.41...,  0.17...],</span>
<span class="go">       [-1.33..., -1.48...,  0.35...,  0.15...]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>                        
<span class="go">array([ 0.92...,  0.05...])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                  
<span class="go">0.97...</span>
</pre></div>
</div>
<p>Let us project the iris dataset along those first 3 dimensions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>The dataset has been &#8220;normalized&#8221;, which means that the data is now centered on
both components with unit variance:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                                   
<span class="go">array([ -1.44...e-15,   1.73...e-15])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_pca</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">array([ 1.,  1.])</span>
</pre></div>
</div>
<p>Furthermore the samples components do no longer carry any linear
correlation:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X_pca</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>                                 
<span class="go">array([[  1.00...e+00,  -7.58...e-16],</span>
<span class="go">       [ -7.58...e-16,   1.00...e+00]])</span>
</pre></div>
</div>
<p>And visualize the dataset using <tt class="docutils literal"><span class="pre">pylab</span></tt>, for instance by defining the
following utility function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">cycle</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">plot_2D</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_names</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">colors</span> <span class="o">=</span> <span class="n">cycle</span><span class="p">(</span><span class="s">&#39;rgbcmykw&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">target_ids</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">target_ids</span><span class="p">,</span> <span class="n">colors</span><span class="p">,</span> <span class="n">target_names</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">pl</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>Calling <tt class="docutils literal"><span class="pre">plot_2D(X_pca,</span> <span class="pre">iris.target,</span> <span class="pre">iris.target_names)</span></tt> will
display the following:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/iris_pca_2d.png"><img alt="2D PCA projection of the iris dataset" src="images/iris_pca_2d.png" style="width: 520.0px; height: 390.0px;" /></a>
<p class="caption">2D PCA projection of the iris dataset</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The default implementation of PCA computes the SVD of the full
data matrix, which is not scalable when both <tt class="docutils literal"><span class="pre">n_samples</span></tt> and
<tt class="docutils literal"><span class="pre">n_features</span></tt> are big (more that a few thousands).</p>
<p class="last">If you are interested in a number of components that is much
smaller than both <tt class="docutils literal"><span class="pre">n_samples</span></tt> and <tt class="docutils literal"><span class="pre">n_features</span></tt>, consider using
<tt class="docutils literal"><span class="pre">scikits.learn.decomposition.RandomizedPCA</span></tt> instead.</p>
</div>
</div>
<div class="section" id="other-applications-of-dimensionality-reduction">
<h4>2.3.1.2. Other applications of dimensionality reduction<a class="headerlink" href="#other-applications-of-dimensionality-reduction" title="Permalink to this headline">¶</a></h4>
<p>Dimensionality Reduction is not just useful for visualization of
high dimensional datasets. It can also be used as a preprocessing
step (often called data normalization) to help speed up supervised
machine learning methods that are not computationally efficient with high
<tt class="docutils literal"><span class="pre">n_features</span></tt> such as SVM classifiers with gaussian kernels for
instance or that do not work well with linearly correlated features.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><tt class="docutils literal"><span class="pre">scikit-learn</span></tt> also features an implementation of Independant
Component Analysis (ICA) and work is under way to implement common
manifold extraction strategies.</p>
</div>
</div>
</div>
<div class="section" id="clustering">
<h3>2.3.2. Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h3>
<p>Clustering is the task of gathering samples into groups of similar
samples according to some predefined similarity or dissimilarity
measure (such as the Euclidean distance).</p>
<p>For instance let us reuse the output of the 2D PCA of the iris
dataset and try to find 3 groups of samples using the simplest
clustering algorithm (KMeans):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scikits.learn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>                     
<span class="go">array([[ 0.33...,  0.90...],</span>
<span class="go">       [ 0.99..., -0.69...],</span>
<span class="go">       [-1.28..., -0.43...]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
<span class="go">array([1, 1, 0, 1, 1, 1, 0, 1, 1, 0])</span>
</pre></div>
</div>
<p>We can plot the assigned cluster labels instead of the target names
with:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">plot_2D</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="p">[</span><span class="s">&quot;c0&quot;</span><span class="p">,</span> <span class="s">&quot;c1&quot;</span><span class="p">,</span> <span class="s">&quot;c2&quot;</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/iris_pca_2d_kmeans.png"><img alt="KMeans cluster assignements on 2D PCA iris data" src="images/iris_pca_2d_kmeans.png" style="width: 520.0px; height: 390.0px;" /></a>
<p class="caption">KMeans cluster assignements on 2D PCA iris data</p>
</div>
<div class="section" id="notable-implementations-of-clustering-models">
<h4>2.3.2.1. Notable implementations of clustering models<a class="headerlink" href="#notable-implementations-of-clustering-models" title="Permalink to this headline">¶</a></h4>
<p>The following are two well-known clustering algorithms. Like most
unsupervised learning models in the scikit, they expect the data
to be clustered to have the shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt>:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.cluster.KMeans</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">The simplest, yet effective clustering algorithm. Needs to be
provided with the number of clusters in advance, and assumes that the
data is normalized as input (but use a PCA model as preprocessor).</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.cluster.MeanShift</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Can find better looking clusters than KMeans but is not scalable
to high number of samples.</td>
</tr>
</tbody>
</table>
<p>Other clustering algorithms do not work with a data matrix with
shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt> but directly with a precomputed
affinity matrix with shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_samples)</span></tt>:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.cluster.AffinityPropagation</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">Clustering algorithm based on message passing between data points.</td>
</tr>
<tr class="field"><th class="field-name" colspan="2"><tt class="docutils literal"><span class="pre">scikits.learn.cluster.SpectralClustering</span></tt>:</th></tr>
<tr class="field"><td>&nbsp;</td><td class="field-body">KMeans applied to a projection of the normalized graph Laplacian:
finds normalized graph cuts if the affinity matrix is interpreted
as an adjacency matrix of a graph.</td>
</tr>
</tbody>
</table>
<p>Hierarchical clustering is being implemented in a branch that is
likely to be merged into master before the release of <tt class="docutils literal"><span class="pre">scikit-learn</span></tt>
0.8.</p>
</div>
<div class="section" id="applications-of-clustering">
<h4>2.3.2.2. Applications of clustering<a class="headerlink" href="#applications-of-clustering" title="Permalink to this headline">¶</a></h4>
<p>Here are some common applications of clustering algorithms:</p>
<ul class="simple">
<li>Building customer profiles for market analysis</li>
<li>Grouping related web news (e.g. Google News) and websearch results</li>
<li>Grouping related stock quotes for investment portfolio management</li>
<li>Can be used as a preprocessing step for recommender systems</li>
<li>Can be used to build a code book of prototype samples for unsupervised
feature extraction for supervised learning algorithms</li>
</ul>
</div>
</div>
</div>
<div class="section" id="linearly-separable-data">
<h2>2.4. Linearly separable data<a class="headerlink" href="#linearly-separable-data" title="Permalink to this headline">¶</a></h2>
<p>Some supervised learning problems can be solved by very simple
models (called generalized linear models) depending on the data.
Others simply don&#8217;t.</p>
<p>To grasp the difference between the two cases, run the interactive
example from the <tt class="docutils literal"><span class="pre">examples</span></tt> folder of the <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> source
distribution:</p>
<div class="highlight-python"><pre>% python $SKL_HOME/examples/applications/svm_gui.py</pre>
</div>
<ol class="arabic simple">
<li>Put some data points belonging to one of the two target classes
(&#8216;white&#8217; or &#8216;black&#8217;) using left click and right click.</li>
<li>Choose some parameters of a Support Vector Machine to be trained on
this toy dataset (<tt class="docutils literal"><span class="pre">n_samples</span></tt> is the number of clicks, <tt class="docutils literal"><span class="pre">n_features</span></tt>
is 2).</li>
<li>Click the Fit but to train the model and see the decision boundary.
The accurracy of the model is displayed on stdout.</li>
</ol>
<p>The following figures demonstrate one case where a linear model can
perfectly separate the two classes while the other is not linearly
separable (a model with a gaussian kernel is required in that case).</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/linearly_separable_data.png"><img alt="images/linearly_separable_data.png" src="images/linearly_separable_data.png" style="width: 489.75px; height: 455.25px;" /></a>
<p class="caption">Linear Support Vector Machine trained to perfectly separate 2
sets of data points labeled as white and black in a 2D space.</p>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="images/non_linearly_separable_data.png"><img alt="images/non_linearly_separable_data.png" src="images/non_linearly_separable_data.png" style="width: 490.5px; height: 456.0px;" /></a>
<p class="caption">Support Vector Machine with gaussian kernel trained to separate
2 sets of data points labeled as white and black in a 2D space.
This dataset would not have been seperated by a simple linear
model.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Exercise:</th><td class="field-body"><p class="first">Fit a model that is able to solve the XOR problem using the GUI:
the XOR problem is composed of 4 samples:</p>
<blockquote>
<div><ul class="simple">
<li>2 white samples in the top-left and bottom-right corners</li>
<li>2 black samples in the bottom-left and top-right corners</li>
</ul>
</div></blockquote>
<p><strong>Question</strong>: is the XOR problem linearly separable?</p>
</td>
</tr>
<tr class="field"><th class="field-name">Exercise:</th><td class="field-body"><p class="first last">Construct a problem with less than 10 points where the predictive
accuracy of the best linear model is 50%.</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="training-set-test-set-and-overfitting">
<h2>2.5. Training set, test set and overfitting<a class="headerlink" href="#training-set-test-set-and-overfitting" title="Permalink to this headline">¶</a></h2>
<p>The most common mistake beginners do when training statistical
models is to evaluate the quality of the model on the same data
used for fitting the model:</p>
<blockquote>
<div>If you do this, <strong>you are doing it wrong!</strong></div></blockquote>
<div class="section" id="the-overfitting-issue">
<h3>2.5.1. The overfitting issue<a class="headerlink" href="#the-overfitting-issue" title="Permalink to this headline">¶</a></h3>
<p>The problem lies in the fact that some models can be subject to the
<strong>overfitting</strong> issue: they can <strong>learn the training data by heart</strong>
without generalizing. The symptoms are:</p>
<blockquote>
<div><ul class="simple">
<li>the predictive accurracy on the data used for training can be excellent
(sometimes 100%)</li>
<li>however, the models do little better than random prediction when facing
new data that was not part of the training set</li>
</ul>
</div></blockquote>
<p>If you evaluate your model on your training data you won&#8217;t be able to tell
whether your model is overfitting or not.</p>
</div>
<div class="section" id="solutions-to-overfitting">
<h3>2.5.2. Solutions to overfitting<a class="headerlink" href="#solutions-to-overfitting" title="Permalink to this headline">¶</a></h3>
<p>The solution to this issue is twofold:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Split your data into two sets to detect overfitting situations:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>one for training and model selection: the <strong>training set</strong></li>
<li>one for evaluation: the <strong>test set</strong></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>Avoid overfitting by using simpler models (e.g. linear classifiers
instead of gaussian kernel SVM) or by increasing the regularization
parameter of the model if available (see the docstring of the
model for details)</li>
</ol>
</div></blockquote>
<p>An even better option when experimenting with classifiers is to divide
the data into three sets: training, testing and holdout. You can then
optimize your features, settings and algorithms for the testing set until
they seem good enough, and finally test on the holdout set (perhaps after
adding the test set to the training set).</p>
<p>When the amount of labeled data available is small, it may not be feasible
to construct training and test sets. In that case, use <strong>cross validation</strong>:
divide the dataset into ten parts of (roughly) equal size, then for each of
these ten parts, train the classifier on the other nine and test on the
held-out part.</p>
</div>
<div class="section" id="measuring-classification-performance-on-a-test-set">
<h3>2.5.3. Measuring classification performance on a test set<a class="headerlink" href="#measuring-classification-performance-on-a-test-set" title="Permalink to this headline">¶</a></h3>
<p>Here is an example on you to split the data on the iris dataset.</p>
<p>First we need to shuffle the order of the samples and the target
to ensure that all classes are well represented on both sides of
the split:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">array([ 73,  18, 118,  78,  76,  31,  64, 141,  68,  82])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</pre></div>
</div>
<p>We can now split the data using a 2/3 - 1/3 ratio:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">split</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">split</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100, 4)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(50, 4)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(100,)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(50,)</span>
</pre></div>
</div>
<p>We can now re-train a new linear classifier on the training set only:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>To evaluate its quality we can compute the average number of correct
classifications on the test set:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>This shows that the model has a predictive accurracy of 100% which
means that the classification model was perfectly capable of
generalizing what was learned from the training set to the test
set: this is rarely so easy on real life datasets as we will see
in the following chapter.</p>
</div>
</div>
<div class="section" id="key-takeaway-points">
<h2>2.6. Key takeaway points<a class="headerlink" href="#key-takeaway-points" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Build <tt class="docutils literal"><span class="pre">X</span></tt> (features vectors) with shape <tt class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></tt></li>
<li>Supervised learning: <tt class="docutils literal"><span class="pre">clf.fit(X,</span> <span class="pre">y)</span></tt> and then <tt class="docutils literal"><span class="pre">clf.predict(X_new)</span></tt><ul>
<li>Classification: <tt class="docutils literal"><span class="pre">y</span></tt> is an array of integers</li>
<li>Regression: <tt class="docutils literal"><span class="pre">y</span></tt> is an array of floats</li>
</ul>
</li>
<li>Unsupervised learning: <tt class="docutils literal"><span class="pre">clf.fit(X)</span></tt><ul>
<li>Dimensionality Reduction with <tt class="docutils literal"><span class="pre">clf.transform(X_new)</span></tt><ul>
<li>for visualization</li>
<li>for scalability</li>
</ul>
</li>
<li>Clustering finds group id for each sample</li>
</ul>
</li>
<li>Some models work much better with data normalized with PCA</li>
<li>Simple linear models can fail completely (non linearly separable data)</li>
<li>Simple linear models often very useful in practice (esp. with
large <tt class="docutils literal"><span class="pre">n_features</span></tt>)</li>
<li>Before starting to train a model: split train / test data:<ul>
<li>use training set for model selection and fitting</li>
<li>use test set for model evaluation</li>
<li>use cross-validation when your dataset is small</li>
</ul>
</li>
<li>Complex models can overfit (learn by heart) the training data and
fail to generalize correctly on test data:<ul>
<li>try simpler models first</li>
<li>tune the regularization parameter on a validation set</li>
</ul>
</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        <p style="text-align: center">This tutorial is for scikit-learn
        version 0.9<p>
        &copy; 2010–2011, scikits.learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    </div>
  </body>
</html>